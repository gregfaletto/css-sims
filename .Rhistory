rm(list=ls())
n <- 100
nsims <- 10
beta_hat_1 <- rep(as.numeric(NA), nsims)
beta_hat_2 <- rep(as.numeric(NA), nsims)
beta_star <- 1
for(i in 1:nsims){
X <- rnorm(n)
T_feat <- rbinom(n, size=1)
epsilon <- rnorm(n)
y <- beta_star*X + T_Feat + T_feat*X + epsilon
df <- data.frame(y=y, t=T_feat, x=X)
model1 <- lm(y ~ x, df)
model2 <- lm(y ~ x + t, df)
}
rm(list=ls())
n <- 100
nsims <- 10
beta_hat_1 <- rep(as.numeric(NA), nsims)
beta_hat_2 <- rep(as.numeric(NA), nsims)
beta_star <- 1
for(i in 1:nsims){
X <- rnorm(n)
T_feat <- rbinom(n, size=1, prob=.5)
epsilon <- rnorm(n)
y <- beta_star*X + T_Feat + T_feat*X + epsilon
df <- data.frame(y=y, t=T_feat, x=X)
model1 <- lm(y ~ x, df)
model2 <- lm(y ~ x + t, df)
}
?rbinom
for(i in 1:nsims){
X <- rnorm(n)
T_feat <- rbinom(n, size=1, prob=.5)
epsilon <- rnorm(n)
y <- beta_star*X + T_feat + T_feat*X + epsilon
df <- data.frame(y=y, t=T_feat, x=X)
model1 <- lm(y ~ x, df)
model2 <- lm(y ~ x + t, df)
}
coef(model1)
coelf(model1)$x
coelf(model1)["x"]
coef(model1)["x"]
coef(model2)["x"]
rm(list=ls())
n <- 100
nsims <- 1000
beta_hat_1 <- rep(as.numeric(NA), nsims)
beta_hat_2 <- rep(as.numeric(NA), nsims)
beta_star <- 1
for(i in 1:nsims){
X <- rnorm(n)
T_feat <- rbinom(n, size=1, prob=.5)
epsilon <- rnorm(n)
y <- beta_star*X + T_feat + T_feat*X + epsilon
df <- data.frame(y=y, t=T_feat, x=X)
model1 <- lm(y ~ x, df)
model2 <- lm(y ~ x + t, df)
beta_hat_1[i] <- coef(model1)["x"]
beta_hat_2[i] <- coef(model2)["x"]
}
print("beta_hat_1:")
print(mean(beta_hat_1))
print(sum((beta_hat_1 - beta_star)^2)/nsims)
print("beta_hat_2:")
print(mean(beta_hat_2))
print(sum((beta_hat_2 - beta_star)^2)/nsims)
rm(list=ls())
n <- 100
nsims <- 1000
beta_hat_1 <- rep(as.numeric(NA), nsims)
beta_hat_2 <- rep(as.numeric(NA), nsims)
beta_star <- 1
for(i in 1:nsims){
T_feat <- rbinom(n, size=1, prob=.5)
X <- .5*rnorm(n) + T_feat*.5*rnorm(n)
epsilon <- rnorm(n)
y <- beta_star*X + T_feat + T_feat*X + epsilon
df <- data.frame(y=y, t=T_feat, x=X)
model1 <- lm(y ~ x, df)
model2 <- lm(y ~ x + t, df)
beta_hat_1[i] <- coef(model1)["x"]
beta_hat_2[i] <- coef(model2)["x"]
}
print("beta_hat_1:")
print(mean(beta_hat_1))
print(sum((beta_hat_1 - beta_star)^2)/nsims)
print("beta_hat_2:")
print(mean(beta_hat_2))
print(sum((beta_hat_2 - beta_star)^2)/nsims)
rm(list=ls())
n <- 100
nsims <- 1000
beta_hat_1 <- rep(as.numeric(NA), nsims)
beta_hat_2 <- rep(as.numeric(NA), nsims)
beta_star <- 1
for(i in 1:nsims){
# T_feat <- rbinom(n, size=1, prob=.5)
T_feat <- rnorm(n)
X <- .5*rnorm(n) + .5*T_feat
epsilon <- rnorm(n)
y <- beta_star*X + T_feat + T_feat*X + epsilon
df <- data.frame(y=y, t=T_feat, x=X)
model1 <- lm(y ~ x, df)
model2 <- lm(y ~ x + t, df)
beta_hat_1[i] <- coef(model1)["x"]
beta_hat_2[i] <- coef(model2)["x"]
}
print("beta_hat_1:")
print(mean(beta_hat_1))
print(sum((beta_hat_1 - beta_star)^2)/nsims)
print("beta_hat_2:")
print(mean(beta_hat_2))
print(sum((beta_hat_2 - beta_star)^2)/nsims)
rm(list=ls())
n <- 100
nsims <- 1000
beta_hat_1 <- rep(as.numeric(NA), nsims)
beta_hat_2 <- rep(as.numeric(NA), nsims)
beta_star <- 1
for(i in 1:nsims){
# T_feat <- rbinom(n, size=1, prob=.5)
T_feat <- rnorm(n)
# X <- .5*rnorm(n) + .5*T_feat
X <- rnorm(n)
epsilon <- rnorm(n)
y <- beta_star*X + T_feat + T_feat*X + epsilon
df <- data.frame(y=y, t=T_feat, x=X)
model1 <- lm(y ~ x, df)
model2 <- lm(y ~ x + t, df)
beta_hat_1[i] <- coef(model1)["x"]
beta_hat_2[i] <- coef(model2)["x"]
}
print("beta_hat_1:")
print(mean(beta_hat_1))
print(sum((beta_hat_1 - beta_star)^2)/nsims)
print("beta_hat_2:")
print(mean(beta_hat_2))
print(sum((beta_hat_2 - beta_star)^2)/nsims)
(291.46 + 40.06)/2
1715.22
6946.51-1715.22-1500
1041.69+3731.29
3731.29+1041.69-100
3731.29+1041.69-100-4282.50
?is.data.frame
library(MASS)
fractions(2/18 - 1/256)
fractions(4*(1/3 - 1/5))
fractions(4*(1/4 - 1/6))
fractions(1/3 - (8/15)^2)
package_version("testthat")
library(testthat)
package_version("testthat")
packageVersion("testthat")
remotes::install_github("jacobbien/litr-project", subdir = "litr")
rm(list=ls())
rmarkdown::draft("create-rhello.Rmd", template = "make-an-r-package", package = "litr")
rmarkdown::draft("create-rhello.Rmd", template = "make-an-r-package", package = "litr")
?case_when
library(tidyverse)
p <- c(4/16, 3/16, 2/16, 1/16, 3/16, 2/16, 1/16)
sum(p)
x <- c(0:3, -1:-3)
x
x %*% p
(x - 0)^2 %*% p
sqrt((x - 0)^2 %*% p)
2.5*16
500*1/2400 + 4*4/2400 + 10*10/2400
library(MASS)
fractions(500*1/2400 + 4*4/2400 + 10*10/2400)
77*8
x <- c(500, 4, 10, 0)
p <- c(1/2400, 4/2400, 10/2400, 2385/2400)
sum(p)
(x - 77/300)^2 %*% p
sqrt((x - 77/300)^2 %*% p)
x %*% p
77/300*5
x <- c(-1000, 0, 1000, 2000, 3000)
p <- c(0.13, 0.15, .24, 0.35, 0.13)
x %*% p
sum(p)
(1 - .45)^10
1 - (1 - .45)^10 - 10*.45*(1 - .45)^9
?binomcdf
?rbinom
pbinom(q=5, size=10, prob=0.45)
pbinom(q=0, size=10, prob=0.45)
pbinom(q=1, size=10, prob=0.45)
(1 - .45)^10 + 10*.45*(1 - .45)^9
dbinom(x=3, size=5, prob=0.25)
qnorm(p=0.7, mean=125, sd=6.5)
1 - .7^5 - 5*.3*.7^4
?rgeom
(1 - .19)^4*.19
1/.19
1 - pgeom(10, .19)
4*.6^3*.4 + .6^4
15*.6
sqrt(15*.6*.4)
5400+3070+2200-1720
5400+3070+2200-1720-6033
81346 - 75323
564-69
library(cssr)
data <- genClusteredData(n = 200, # Sample size
p = 100, # Number of features
cluster_size = 10, # Number of features in a cluster correlated with a latent variable
k_unclustered = 10, # Number of unclustered features that influence y
snr = 3 # Signal-to-noise ratio in the response y generated from the data.
)
X <- data$X
y <- data$y
output <- cssSelect(X, y)
rm(list=ls())
data <- genClusteredData(n = 80, # Sample size
p = 40, # Number of features
cluster_size = 10, # Number of features in a cluster correlated with a latent variable
k_unclustered = 10, # Number of unclustered features that influence y
snr = 3 # Signal-to-noise ratio in the response y generated from the data.
)
X <- data$X
y <- data$y
output <- cssSelect(X, y)
output <- cssSelect(X, y)
output$selected_feats
clus_output <- cssSelect(X, y, clusters=list("Z_cluster"=1:10))
clus_output <- cssSelect(X, y, clusters=list("Z_cluster"=1:10))
clus_output$selected_feats
clus_output$selected_clusts
clusters <- list("Z_clust"=1:10, 50:55)
# Wrapper functions (easy!)
n_test <- 50
n <- 200
p <- 100
testx <- matrix(rnorm(n_test*p), nrow=n_test, ncol=p)
cssPredict(X, y, testx, clusters)
clusters <- list("Z_clust"=1:10, 36:40)
# Wrapper functions (easy!)
n_test <- 50
n <- 80
p <- 40
testx <- matrix(rnorm(n_test*p), nrow=n_test, ncol=p)
# cssPredict(X, y, testx, clusters)
cssPredict(X, y, testx, clusters)
n_test <- 50
n <- 200
p <- 40
testx <- matrix(rnorm(n_test*p), nrow=n_test, ncol=p)
cssPredict(X, y, testx, clusters)
inds <- 1:round(n/2)
lambda <- getLassoLambda(X[setdiff(1:n, inds), ], y[setdiff(1:n, inds)])
lambda <- getLassoLambda(X, y)
lambda
results <- css(X=X, y=y, lambda=lambda
, clusters=clusters
# , clusters=list()
# , clusters=1:10
# , sampling.type = "SS"
# B = 100,
# , prop_feats_remove = .5
, train_inds = inds
)
inds <- 1:round(n/2)
results <- css(X=X, y=y, lambda=lambda
, clusters=clusters
# , clusters=list()
# , clusters=1:10
# , sampling.type = "SS"
# B = 100,
# , prop_feats_remove = .5
, train_inds = inds
)
nrow(X)
n
inds <- 1:40
results <- css(X=X, y=y, lambda=lambda
, clusters=clusters
# , clusters=list()
# , clusters=1:10
# , sampling.type = "SS"
# B = 100,
# , prop_feats_remove = .5
, train_inds = inds
)
str(results)
predictions <- results |> getCssPreds(testX = testx, weighting="sparse",
cutoff=0.3
, min_num_clusts=1
, max_num_clusts=3
)
predictions
train_x <- matrix(rnorm(n_test*p), nrow=n_test, ncol=p)
train_y <- rnorm(n_test)
preds2 <- results |> getCssPreds(testX = testx, weighting=w,
cutoff=c, min_num_clusts=1, max_num_clusts=3,
trainX=train_x
, trainY=train_y
)
preds2 <- results |> getCssPreds(testX = testx, weighting="sparse",
cutoff=0.3, min_num_clusts=1, max_num_clusts=3,
trainX=train_x
, trainY=train_y)
preds2
selections <- results |> getCssSelections(weighting=w, cutoff=c
# , min_num_clusts=1
# , max_num_clusts=3
)
selections <- results |> getCssSelections(weighting="sparse", cutoff=0.3
# , min_num_clusts=1
# , max_num_clusts=3
)
str(selections)
selections$selected_clusts
selections$selected_feats
results |> print.cssr(cutoff=c, min_num_clusts=1, max_num_clusts=3)
print(results)
print(results, cutoff=0.3, max_num_clusts=5)
x_design <- results |> getCssDesign(testx, weighting=w, cutoff=c, min_num_clusts=1, max_num_clusts=3)
x_design <- results |> getCssDesign(testx, weighting="weighted_avg", cutoff=0.3,
min_num_clusts=1, max_num_clusts=3)
str(x_design)
rm(list=ls())
data <- genClusteredData(n = 80, # Sample size
p = 40, # Number of features
cluster_size = 10, # Number of features in a cluster correlated with a latent variable
k_unclustered = 10, # Number of unclustered features that influence y
snr = 3 # Signal-to-noise ratio in the response y generated from the data.
)
X <- data$X
y <- data$y
output <- cssSelect(X, y)
output$selected_feats
library(cssr)
data <- genClusteredData(n = 80, # Sample size
p = 40, # Number of features
cluster_size = 10, # Number of features in a cluster correlated with a latent variable
k_unclustered = 10, # Number of unclustered features that influence y
snr = 3 # Signal-to-noise ratio in the response y generated from the data.
)
X <- data$X
y <- data$y
output <- cssSelect(X, y)
output$selected_feats
clus_output <- cssSelect(X, y, clusters=list("Z_cluster"=1:10))
clus_output$selected_feats
clus_output$selected_clusts
clusters <- list("Z_clust"=1:10, 36:40)
# Wrapper functions (easy!)
n_test <- 50
n <- 80
p <- 40
testx <- matrix(rnorm(n_test*p), nrow=n_test, ncol=p)
cssPredict(X, y, testx, clusters)
# Get a good lambda
lambda <- getLassoLambda(X, y)
# clusters <- list(1:10, 46:40)
# clusters <- 1:10
inds <- 1:40
results <- css(X=X, y=y, lambda=lambda
, clusters=clusters
# , clusters=list()
# , clusters=1:10
# , sampling.type = "SS"
# B = 100,
# , prop_feats_remove = .5
, train_inds = inds
)
str(results)
predictions <- results |> getCssPreds(testX = testx, weighting="sparse",
cutoff=0.3
, min_num_clusts=1
, max_num_clusts=3
)
predictions
train_x <- matrix(rnorm(n_test*p), nrow=n_test, ncol=p)
train_y <- rnorm(n_test)
preds2 <- results |> getCssPreds(testX = testx, weighting="sparse",
cutoff=0.3, min_num_clusts=1, max_num_clusts=3,
trainX=train_x
, trainY=train_y)
preds2
selections <- results |> getCssSelections(weighting="sparse", cutoff=0.3
# , min_num_clusts=1
# , max_num_clusts=3
)
str(selections)
selections$selected_clusts
selections$selected_feats
print(results, cutoff=0.3, max_num_clusts=5)
x_design <- results |> getCssDesign(testx, weighting="weighted_avg", cutoff=0.3,
min_num_clusts=1, max_num_clusts=3)
str(x_design)
?rowMeans
x_design
rowMeans(x_design)
str(x_design)
str(rowMeans(x_design))
?setdiff
remotes::install_github("jacobbien/litr-project", subdir = "litr",force=TRUE)
setwd("/Users/gregfaletto/Documents/GitHub/css-sims")
source("plant.R")
e <- evals(plant_sim)
print("done! converting to data.frame...")
t0 <- Sys.time()
edf <- as.data.frame(e)
str(edf)
n_methods <- length(unique(edf$Method))
n_methods
MIN_COUNT <- 10
setwd(dir_funcs)
source(file="simFunctions.R")
source("toy_ex_slide_funcs.R")
source("model_functions.R")
source("method_functions.R")
source("eval_functions.R")
setwd(wd)
### Generate figures
results <- genPlotDfPlant(plant_sim, coarseness=coarseness)
results_df <- results$results_df
n_methods <- results$n_methods
### Figure 4 (previously Figure 5) (known clusters)
fig_4_left <- createLossesPlot3(results_df[!(results_df$Method %in%
nameMap(c("SS_CSS_sparse_cssr_plant", "SS_CSS_avg_cssr_plant"))), ],
n_methods - 2, plot_errors=FALSE, max_model_size=p_max)
fig_4_mid <- createNSBStabPlot2(results_df[!(results_df$Method %in%
nameMap(c("SS_CSS_sparse_cssr_plant", "SS_CSS_avg_cssr_plant"))), ],
plot_errors=FALSE)
fig_4_right <- createStabMSEPlot2(results_df[!(results_df$Method %in%
nameMap(c("SS_CSS_sparse_cssr_plant", "SS_CSS_avg_cssr_plant"))), ],
n_methods - 2, plot_errors=FALSE)
# 2. Save the legend
#+++++++++++++++++++++++
legend <- get_legend(fig_4_left + theme(legend.direction="horizontal"))
# 3. Remove the legend from the box plot
#+++++++++++++++++++++++
fig_4_left <- fig_4_left + theme(legend.position="none")
fig_4_mid <- fig_4_mid + theme(legend.position="none")
fig_4_right <- fig_4_right + theme(legend.position="none")
# 4. Arrange ggplot2 graphs with a specific width
fig_4 <- grid.arrange(fig_4_left, fig_4_mid, fig_4_right, legend, ncol=3,
nrow = 2, layout_matrix = rbind(c(1, 2, 3), c(4, 4, 4)),
widths = c(1.8, 1.8, 1.8), heights = c(2.5, 0.2))
fig_4 <- cowplot::ggdraw(fig_4) +
theme(plot.background = element_rect(fill="white", color = NA))
print(fig_4)
saveFigure2(subdir="figures", plot=fig_4, size="large",
filename="fig_real_data.pdf")
### Versions of Figure 4 plots with all methods (for supplement)
fig_4_supp_left <- createLossesPlot3(results_df, n_methods,
plot_errors=FALSE, max_model_size=p_max)
saveFigure2(subdir="figures", plot=fig_4_supp_left, size="xmlarge",
filename="real_data_mse_supp.pdf")
fig_4_supp_mid <- createNSBStabPlot2(results_df, plot_errors=FALSE)
saveFigure2(subdir="figures", plot=fig_4_supp_mid, size="xmlarge",
filename="real_data_stab_supp.pdf")
fig_4_supp_right <- createStabMSEPlot2(results_df, n_methods, plot_errors=FALSE)
saveFigure2(subdir="figures", plot=fig_4_supp_right, size="xmlarge",
filename="real_data_mse_stab_supp.pdf")
setwd(dir_funcs)
source(file="simFunctions.R")
source("toy_ex_slide_funcs.R")
source("model_functions.R")
source("method_functions.R")
source("eval_functions.R")
setwd(wd)
### Generate figures
results <- genPlotDfPlant(plant_sim, coarseness=coarseness)
results_df <- results$results_df
n_methods <- results$n_methods
### Figure 4 (previously Figure 5) (known clusters)
fig_4_left <- createLossesPlot3(results_df[!(results_df$Method %in%
nameMap(c("SS_CSS_sparse_cssr_plant", "SS_CSS_avg_cssr_plant"))), ],
n_methods - 2, plot_errors=FALSE, max_model_size=p_max)
fig_4_mid <- createNSBStabPlot2(results_df[!(results_df$Method %in%
nameMap(c("SS_CSS_sparse_cssr_plant", "SS_CSS_avg_cssr_plant"))), ],
plot_errors=FALSE)
fig_4_right <- createStabMSEPlot2(results_df[!(results_df$Method %in%
nameMap(c("SS_CSS_sparse_cssr_plant", "SS_CSS_avg_cssr_plant"))), ],
n_methods - 2, plot_errors=FALSE)
# 2. Save the legend
#+++++++++++++++++++++++
legend <- get_legend(fig_4_left + theme(legend.direction="horizontal"))
# 3. Remove the legend from the box plot
#+++++++++++++++++++++++
fig_4_left <- fig_4_left + theme(legend.position="none")
fig_4_mid <- fig_4_mid + theme(legend.position="none")
fig_4_right <- fig_4_right + theme(legend.position="none")
# 4. Arrange ggplot2 graphs with a specific width
fig_4 <- grid.arrange(fig_4_left, fig_4_mid, fig_4_right, legend, ncol=3,
nrow = 2, layout_matrix = rbind(c(1, 2, 3), c(4, 4, 4)),
widths = c(1.8, 1.8, 1.8), heights = c(2.5, 0.2))
fig_4 <- cowplot::ggdraw(fig_4) +
theme(plot.background = element_rect(fill="white", color = NA))
print(fig_4)
saveFigure2(subdir="figures", plot=fig_4, size="large",
filename="fig_real_data.pdf")
### Versions of Figure 4 plots with all methods (for supplement)
fig_4_supp_left <- createLossesPlot3(results_df, n_methods,
plot_errors=FALSE, max_model_size=p_max)
saveFigure2(subdir="figures", plot=fig_4_supp_left, size="xmlarge",
filename="real_data_mse_supp.pdf")
fig_4_supp_mid <- createNSBStabPlot2(results_df, plot_errors=FALSE)
saveFigure2(subdir="figures", plot=fig_4_supp_mid, size="xmlarge",
filename="real_data_stab_supp.pdf")
fig_4_supp_right <- createStabMSEPlot2(results_df, n_methods, plot_errors=FALSE)
saveFigure2(subdir="figures", plot=fig_4_supp_right, size="xmlarge",
filename="real_data_mse_stab_supp.pdf")
setwd("/Users/gregfaletto/Documents/GitHub/css-sims")
